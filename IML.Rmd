---
title: "ML Bias, Interpretability - Kaggler Perspective & Recommendations"
output:
  html_document:
    number_sections: false
    fig_caption: true
    toc: true
    fig_width: 9
    fig_height: 6
    theme: united
    highlight: zenburn
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Do Computers Lie?

We have been constantly told this statement "Computers don't lie". Yes in fact Computers don't lie, but neither does it speak the truth. A computer does what its Master programs it to do. Similarly, A model wouldn't lie unless the Machine Learning Engineer doesn't want it to lie. 


### Machine Bias

There was a nice episode of the podcast [You are not so smart](https://youarenotsosmart.com/2017/11/20/yanss-115-how-we-transferred-our-biases-into-our-machines-and-what-we-can-do-about-it/) came out last year. This is an excerpt from it:

*“I want a machine-learning algorithm to learn what tumors looked like in the past, and I want it to become biased toward selecting those kind of tumors in the future,” explains philosopher Shannon Vallor at Santa Clara University.  “But I don’t want a machine-learning algorithm to learn what successful engineers and doctors looked like in the past and then become biased toward selecting those kinds of people when sorting and ranking resumes.”*


### The Problem

Machine Bias can occur due to a lot of factors but a few to name is:

* Biased Training Dataset
* Bias Variable in the Feature Space
* BlackBox Modelling of not understanding what's going on with the Model 

Below is an example of how Google Translate, when translated the following text to a Gender-neutral langauge and back to English - applies its bias (primarily due to the nature of biased Training Dataset)

![img](https://image-store.slidesharecdn.com/3b4968dd-1035-430a-823e-33f10c978343-original.png)

### The Solution

The first step of finding solution to any problem is accepting **The Problem exists**. Let's accept that fact and see how to use Kaggle Survey results and help the community tackle Machine Bias.

### Libraries

```{r, message = FALSE, warning=FALSE}
suppressPackageStartupMessages(library(tidyverse)) 
suppressPackageStartupMessages(library(highcharter))
suppressPackageStartupMessages(library(DataExplorer))
suppressPackageStartupMessages(library(scales))
suppressPackageStartupMessages(library(cowplot))
suppressPackageStartupMessages(library(viridis))
suppressPackageStartupMessages(library(wordcloud))
suppressPackageStartupMessages(library(tidytext))
suppressPackageStartupMessages(library(RColorBrewer))

pal <- brewer.pal(9,"BuGn")


plotting_missing <- function(df){

  #based on erikbruin's code snippet
    
NAcol <- which(colSums(is.na(df)) > 0)
NAcount <- sort(colSums(sapply(df[NAcol], is.na)), decreasing = TRUE)
NADF <- data.frame(variable=names(NAcount), missing=NAcount)
NADF$PctMissing <- round(((NADF$missing/nrow(df))*100),1)
NADF %>%
    ggplot(aes(x=reorder(variable, PctMissing), y=PctMissing)) +
    geom_bar(stat='identity', fill='red') + coord_flip(y=c(0,110)) +
    labs(x="", y="Percent missing") +
    geom_text(aes(label=paste0(NADF$PctMissing, "%"), hjust=-0.1))
}


```


```{r, message = FALSE, warning=FALSE}
survey <- read_csv("input/multipleChoiceResponses.csv", skip = 1, col_types = cols())
```

### Ignorance is Bliss - but not always! 
  

```{r echo=FALSE, fig.show = 'hide'}

survey %>%  select(contains("How do you perceive the importance of the following topics?"),
                   contains("Does your current employer incorporate machine learning methods into their business?"),
                   starts_with("In which country"),
                   contains("compensation")
) %>% 
#plot_missing() -> p1 
  plotting_missing() -> p1

```

```{r warning=FALSE}

p1  + theme_minimal() + theme(axis.text = element_text(size = 6)) + labs(title = "Ignorance beyond DS") 

```



The above plot is to demonstrate how much these questions that are about Model Fairness / Bias, have been ignored. 

While asking about **Salary made 15% of respondents** to not answer, Questions about **Reproducibility, Explainability and Bias made 37% of respondents** to skip answering. The salary question comparsion is here to show relatively worse questions like this are approached. 


### Reproducibility, Explainability and Bias


```{r warning = FALSE}
survey %>% select(contains("How do you perceive the importance of the following topics?")) %>% 
  gather() %>% 
  mutate(key = str_replace(key,"-","\n")) %>% 
  mutate(key = str_replace(key,"How do you perceive the importance of the following topics?",""),
         key = str_replace(key, regex("\\?"),""), 
         key = str_replace(key, regex("\\-|\\:"),"")) %>% 
  group_by(key) %>% 
  count(value) %>% 
  drop_na() %>% 
  mutate(n = n / sum(n)) %>% 
  ggplot() + geom_col(aes(value,n, fill = key), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = value, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
  scale_fill_viridis(discrete = T, option = "E") +
  facet_wrap(~key) +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "Perception on Reproducibility, Explainability and Model Bias ",
       subtitle = "Percentage",
       x = "Selected Options",
       y = "Percentage of Respondents (other than NAs)")
```

To get a better perspective of the volume of the respondents, below is the same plot as above but with absolute numbers of respondents and their options.

```{r warning=FALSE}
survey %>% select(contains("How do you perceive the importance of the following topics?")) %>% 
  gather() %>% 
  mutate(key = str_replace(key,"-","\n")) %>% 
  mutate(key = str_replace(key,"How do you perceive the importance of the following topics?",""),
         key = str_replace(key, regex("\\?"),""), 
         key = str_replace(key, regex("\\-|\\:"),"")) %>% 
  group_by(key) %>% 
  count(value) %>% 
  drop_na()  %>% 
  
  ggplot() + geom_col(aes(value,n, fill = key), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = value, y = n + 20, label = n),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
  
  scale_fill_viridis(discrete = T, option = "E") +
  facet_wrap(~key) +
  #scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "Perception on Reproducibility, Explainability and Model Bias ",
       subtitle = "Absolute Numbers",
       x = "Selected Options",
       y = "Percentage of Respondents (other than NAs)")
```



**Fairness and Bias:**

* Only close to half (57.4%) of the respondenrts who chose to answer consider Fairness and Bias in ML Algorithm is a **Very important**.

* This is the lowest **Very important** sentiment echoed by the community of all the 3 questions.

* **3.6%** of those who chose to respondent perceive this is **Not at all important**, which is the highest **Not at all important** feeling expressed of all 3 questions.



 
```{r warning=FALSE}
survey %>% select(contains("How do you perceive the importance of the following topics?")) %>% 
  gather() %>% 
  mutate(key = str_replace(key,"How do you perceive the importance of the following topics?",""),
         key = str_replace(key, regex("\\?"),""), 
         key = str_replace(key, regex("\\-"),"")) %>% 
  group_by(key) %>% 
  count(value) %>% 
  mutate(n = percent(n / sum(n)))  %>% 
  spread(value,n) %>% 
  knitr::kable()
```


## Model Bias & Model Fairness 

### Who are they?

*They* refer to those beings who think Fairness and Bias in ML Alggorithm are **Very Important** in Machine Learning. 


```{r}
they <- survey %>% filter(`How do you perceive the importance of the following topics? - Fairness and bias in ML algorithms:` == "Very important")


not_they <- survey %>% filter(`How do you perceive the importance of the following topics? - Fairness and bias in ML algorithms:` != "Very important")

```


### Gender 

```{r warning=FALSE}

they %>% group_by(`What is your gender? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Gender" = `What is your gender? - Selected Choice`) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
   ggplot() + geom_col(aes(Gender,n, fill = Gender), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = Gender, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
    scale_fill_viridis(discrete = T, option = "E") +

  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Gender",
       y = "Percentage of Respondents (other than NAs)") -> p1



not_they %>% group_by(`What is your gender? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Gender" = `What is your gender? - Selected Choice`) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
    ggplot() + geom_col(aes(Gender,n, fill = Gender), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = Gender, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
    scale_fill_viridis(discrete = T, option = "E") +

  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "Not They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Gender",
       y = "Percentage of Respondents (other than NAs)") -> p2


cowplot::plot_grid(p1,p2)



```


Let us a create a new KPI index called `They - Not They Ratio` to give a different perspective to this comparison.


```{r}
they %>% filter(!`What is your gender? - Selected Choice` %in% c("Other")) %>% 
  group_by(`What is your gender? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Degree" = `What is your gender? - Selected Choice`,
         "They" = n) %>%  
  bind_cols(
not_they %>% filter(!`What is your gender? - Selected Choice` %in% c("Other")) %>% 
  group_by(`What is your gender? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Degree1" = `What is your gender? - Selected Choice`,
         "Not They" = n) %>% 
  select(-Degree1)) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("column",hcaes("Degree","T_NT_Ratio")) %>% 
  hc_title(text = "Gender-wise preference - Model Bias and Fairness  ") %>% 
  hc_add_theme(hc_theme_538()) 
  
```


* There is a difference of **5.1 PP** Female Percentage difference between those who perceive **Model Fariness & Bias in ML** is **Very Important** and Others.

* While this could be seen as that **Female Gender** usually gets affected by these Biases, It's also important to realize that **Male Gender** (Kaggler's) don't echo similar sentiment as their female counterpart. After all, A healthy model is what we all want, don't we?

* Using the index that we created `T_NT_Ratio`, we can see that `Female` gender and those who selected other options other than `Male` are above on top of `Male` Kagglers in their perception about **Model Bias and Fairness**


### Age 

```{r warning=FALSE}

they %>% group_by(`What is your age (# years)?`) %>% count() %>% ungroup() %>% 
  rename("Age" = `What is your age (# years)?`) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
   ggplot() + geom_col(aes(Age,n, fill = Age), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = Age, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
    scale_fill_viridis(discrete = T, option = "D") +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Age",
       y = "Percentage of Respondents (other than NAs)") -> p1



not_they %>% group_by(`What is your age (# years)?`) %>% count() %>% ungroup() %>% 
  rename("Age" = `What is your age (# years)?`) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
    ggplot() + geom_col(aes(Age,n, fill = Age), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = Age, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
    scale_fill_viridis(discrete = T, option = "D") +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "Not They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Age",
       y = "Percentage of Respondents (other than NAs)") -> p2


cowplot::plot_grid(p1,p2)



```

Age doesn't seem to give anything straightway, which probably could be due to a lot of different age brackets. Let us try a bit of engineering to club them into two groups < 30 and > 30. 


```{r warning=FALSE}

they %>% 
  mutate(age_grp = ifelse(parse_number(`What is your age (# years)?`) < 30,
                          "Less than 30",
                          "30+")) %>% 
  group_by(age_grp) %>% count() %>% ungroup() %>% 
  rename("Age" = age_grp) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
   ggplot() + geom_col(aes(Age,n, fill = Age), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = Age, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
    scale_fill_viridis(discrete = T, option = "E") +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Age",
       y = "Percentage of Respondents (other than NAs)") -> p1



not_they %>% 
   mutate(age_grp = ifelse(parse_number(`What is your age (# years)?`) < 30,
                          "Less than 30",
                          "30+")) %>% 
  group_by(age_grp) %>% count() %>% ungroup() %>% 
  rename("Age" = age_grp) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
    ggplot() + geom_col(aes(Age,n, fill = Age), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = Age, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
    scale_fill_viridis(discrete = T, option = "E") +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "Not They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Age",
       y = "Percentage of Respondents (other than NAs)") -> p2


cowplot::plot_grid(p1,p2)

```

This plot helps us say that **the younger ones need to be updated with the implications of Model Bias and Fairness** more than their older counterparts. That leads us to another important section of what they do.

### Student vs Professionals


```{r warning=FALSE}

they %>% 
  mutate(title = ifelse(`Select the title most similar to your current role (or most recent title if retired): - Selected Choice` == "Student",
                          "Student",
                          "Professional")) %>% 
  group_by(title) %>% count() %>% ungroup() %>% 
  rename("Title" = title) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
   ggplot() + geom_col(aes(Title,n, fill = Title), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = Title, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
    scale_fill_viridis(discrete = T, option = "C") +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Title",
       y = "Percentage of Respondents (other than NAs)") -> p1


not_they %>% 
  mutate(title = ifelse(`Select the title most similar to your current role (or most recent title if retired): - Selected Choice` == "Student",
                          "Student",
                          "Professional")) %>% 
  group_by(title) %>% count() %>% ungroup() %>% 
  rename("Title" = title) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
    ggplot() + geom_col(aes(Title,n, fill = Title), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = Title, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
  scale_fill_viridis(discrete = T, option = "C") +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "Not They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Title",
       y = "Percentage of Respondents (other than NAs)") -> p2


cowplot::plot_grid(p1,p2)

```

* As with the previouus insight, once again it comes that Students need to be educated with the concepts of Model Bias and Fairness since there is **~4 PP** difference between Students in They vs Not They. 


### Undergraduate Majors


```{r warning=FALSE}





they %>% 
  mutate(UG = ifelse(`Which best describes your undergraduate major? - Selected Choice` %in% c("Computer science (software engineering, etc.)","Information technology, networking, or system administration"),
                          "CS",
                          "Non_CS")) %>% 
  group_by(UG) %>% count() %>% ungroup() %>% 
  rename("UG" = UG) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
   ggplot() + geom_col(aes(UG,n, fill = UG), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = UG, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
  scale_fill_viridis(discrete = T, option = "D") +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Title",
       y = "Percentage of Respondents (other than NAs)") -> p1


not_they %>% 
   mutate(UG = ifelse(`Which best describes your undergraduate major? - Selected Choice` %in% c("Computer science (software engineering, etc.)","Information technology, networking, or system administration"),
                          "CS",
                          "Non_CS")) %>% 
  group_by(UG) %>% count() %>% ungroup() %>% 
  rename("UG" = UG) %>% 
  mutate(n = n / sum(n),
         perc = percent(n)) %>% 
   ggplot() + geom_col(aes(UG,n, fill = UG), stat = "identity", show.legend = FALSE) +
   geom_label(aes(x = UG, y = n - 0.05, label = percent(n)),
           # hjust=0, vjust=0, size = 4, colour = 'black',
            fontface = 'bold') +
  scale_fill_viridis(discrete = T, option = "D") +
  scale_y_continuous(labels = percent_format()) +
  theme_minimal() + 
  theme(axis.text = element_text(angle = 45, size = 6)) +
  labs(title = "Not They",
       subtitle = "Perception on Fairness and Model Bias ",
       x = "Title",
       y = "Percentage of Respondents (other than NAs)") -> p2


cowplot::plot_grid(p1,p2)

```

* Comptuer Science / IT Engineers have a difference of **~ 3.4PP** between **They and Not They**, which shows that's needs more care in the mindset change than Non-CS (even though that's also required) but at least this can help prioritise where to start with any campaigning


### R vs Python vs more


```{r}

they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`What specific programming language do you use most often? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Lang" = `What specific programming language do you use most often? - Selected Choice`,
         "They" = n) %>%  
  bind_cols(
not_they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`What specific programming language do you use most often? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Lang1" = `What specific programming language do you use most often? - Selected Choice`,
         "Not They" = n) %>% 
  select(-Lang1)) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("bar",hcaes("Lang","T_NT_Ratio")) %>% 
  hc_title(text = "Language ordered by They-NotThey Ratio") %>% 
  hc_add_theme(hc_theme_538()) 
  

```




* R users tend to be perceive Model Bias and Fairness as **Very Important** than their Python Counterparts.



### All Countries

```{r warning=FALSE}

they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`In which country do you currently reside?`) %>% count() %>% ungroup() %>% 
  rename("Country" = `In which country do you currently reside?`,
         "They" = n) %>%  
  bind_cols(
not_they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`In which country do you currently reside?`) %>% count() %>% ungroup() %>% 
  rename("Country1" = `In which country do you currently reside?`,
         "Not They" = n) %>% 
  select(-Country1)) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("line",hcaes("Country","T_NT_Ratio")) %>% 
  hc_title(text = "They vs Not They Ratio - Country-wise") %>% 
    hc_add_theme(hc_theme_538())


```




### Countries with at least 100 respondents

```{r warning=FALSE}

they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`In which country do you currently reside?`) %>% count() %>% ungroup() %>% 
  rename("Country" = `In which country do you currently reside?`,
         "They" = n) %>%  
  bind_cols(
not_they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`In which country do you currently reside?`) %>% count() %>% ungroup() %>% 
  rename("Country1" = `In which country do you currently reside?`,
         "Not They" = n) %>% 
  select(-Country1)) %>% 
  filter((They + `Not They`) > 100) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("line",hcaes("Country","T_NT_Ratio")) %>% 
  hc_title(text = "They vs Not They Ratio - Country-wise > 100 respondents") %>% 
    hc_add_theme(hc_theme_538())


```

* **Chile** leads the pack if all the countries are considered otherwise if only the countries with more than 100 respondents are selected, **South Africa** followed by **Nigeria** are leading the pack of a healthy They to Not They ratio.


### Media Sources


```{r}


they %>% 
  dplyr::select(contains("your favorite media sources that report on data science topics")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:22, key = "questions", value = "media") %>% 
 
  #tidyr::replace_na() %>% 
  group_by(media) %>% 
  count() %>% 
  rename("They" = n) %>% 
bind_cols(
 

not_they %>% 
  dplyr::select(contains("your favorite media sources that report on data science topics")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:22, key = "questions", value = "media") %>% 
 
  group_by(media) %>% 
  count() %>% 
  rename("Not They" = n)
) %>% 
  dplyr::select(-media1) %>% 
mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  drop_na() %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("line",hcaes("media","T_NT_Ratio")) %>% 
  hc_title(text = " Model Bias and Fairness - Media Sources - They vs Not They Ratio") %>% 
    hc_add_theme(hc_theme_538())
```

* The above plot makes it very clear that **Podcasts** like **Partiall Derivative**, **Data Skpetic** and **Linear Digression** are places where Kagglers who strongly believe that ML Bias and Fairness get their news from.

* **Nate Silver's 538** makes an entry in the top 5 media sources arranged by `T_NT_Ratio` KPI

* **Kaggle Forums** seemed to need to make more work in terms of encouraging or initiating discussions about **ML Bias and Fairness**


### Work Industry

```{r warning=FALSE}

they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Industry" = `In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`,
         "They" = n) %>%  
  bind_cols(
not_they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Industry1" = `In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`,
         "Not They" = n) %>% 
  select(-Industry1)) %>% 
  #filter((They + `Not They`) > 100) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  filter(!Industry %in% c("3","Other")) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("line",hcaes("Industry","T_NT_Ratio")) %>% 
  hc_title(text = "They vs Not They Ratio - Industry-wise") %>% 
    hc_add_theme(hc_theme_538())


```

* Kagglers in Industries like **Non_Profit/Service** and **Government/Public Service** have been better perception about the importance of Model Fairness and Bias.

* The above plot once again empahsis the importance of Model Bias and Fairness education among students. 
* It's also unhealthy to see places like **Military** and **Internet-based Services** falling behind as those are the places where the model evaluation is crucial and can have serious outcomes. 

 
### Data Scientist?

```{r warning=FALSE}

they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`Do you consider yourself to be a data scientist?`) %>% count() %>% ungroup() %>% 
  rename("DS" = `Do you consider yourself to be a data scientist?`,
         "They" = n) %>%  
  bind_cols(
not_they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`Do you consider yourself to be a data scientist?`) %>% count() %>% ungroup() %>% 
  rename("DS1" = `Do you consider yourself to be a data scientist?`,
         "Not They" = n)# %>% 
 # select(-DS1)
) %>% 
  #filter((They + `Not They`) > 100) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  filter(!DS %in% c("3","Other")) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("line",hcaes("DS","T_NT_Ratio")) %>% 
  hc_title(text = "They vs Not They Ratio - Based on if they consider themsevles a Data Scientist") %>% 
    hc_add_theme(hc_theme_538())


```

* Those who consider themselves to be **Definitely** a data scientist are more likely also to believe about Model Bias and Fairness with those who do not consider themselves a data scientists making the last end of the spectrum with lowest They vs Not They Ratio!


### Type of Data


```{r}


they %>% 
  dplyr::select(contains("types of data")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:11, key = "questions", value = "DataType") %>% 
  #tidyr::replace_na() %>% 
  group_by(DataType) %>% 
  count() %>% 
  rename("They" = n) %>% 
bind_cols(
  not_they %>% 
  dplyr::select(contains("types of data")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:11, key = "questions", value = "DataType") %>% 
  #tidyr::replace_na() %>% 
  group_by(DataType) %>% 
  count() %>% 
  rename("Not They" = n)
) %>% 
  dplyr::select(-DataType1) %>% 
mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("line",hcaes("DataType","T_NT_Ratio")) %>% 
  hc_title(text = "Type of Data - They vs Not They Ratio ") %>% 
    hc_add_theme(hc_theme_538())
```

* Kagglers who use **Genetic Data** lead the table of giving importance to Model Bias 
* Kagglers who use **Image Data** and **Video Data** are least bothered 

### MOOC - Online course Platform


```{r}


they %>% 
  dplyr::select(contains("online platforms have you begun or completed data science courses")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:13, key = "questions", value = "DataType") %>% 
 
  #tidyr::replace_na() %>% 
  group_by(DataType) %>% 
  count() %>% 
  rename("They" = n) %>% 
bind_cols(
 

not_they %>% 
  dplyr::select(contains("online platforms have you begun or completed data science courses")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:13, key = "questions", value = "DataType") %>% 
 
  group_by(DataType) %>% 
  count() %>% 
  rename("Not They" = n)
) %>% 
  dplyr::select(-DataType1) %>% 
mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("column",hcaes("DataType","T_NT_Ratio", color = "T_NT_Ratio")) %>% 
   hc_title(text = " MOOC - Online course platform - They vs Not They Ratio") %>% 
   hc_add_theme(hc_theme_538())
```


* Ironically, **Kaggle Learn** combining with **Coursera** and **Udemy** top the places from where people who don't think Model Bias is **very important** learn Data Science / Machine Learning
* Kagglers who learn DS from **Google Developers**, **Theschool.ai** and **Fast.ai** are the ones who lead the thought process of thinking that Model Bias is **very important**


### Percentage of Data Projects


```{r}

survey %>% 
  select(`Approximately what percent of your data projects involved exploring unfair bias in the dataset and/or algorithm?`) %>% 
  drop_na() %>% 
  group_by(`Approximately what percent of your data projects involved exploring unfair bias in the dataset and/or algorithm?`) %>% 
  rename("Percentage" = `Approximately what percent of your data projects involved exploring unfair bias in the dataset and/or algorithm?`) %>% 
  count() %>% 
  hchart("area",hcaes("Percentage","n")) %>% 
  hc_title(text = "What percent of your data projects involved exploring unfair bias ") %>% 
  hc_add_theme(hc_theme_538()) 
  

```


* As you can see above, There is a very very minimal demand in the workplace to explore unfair ML bias. 
* In fact, only ~ 1.2K respondents of the total ~13K respondents who respondended to this question, have answered to have a need of more than 50% to explore unfair bias in their data projects. (that's less than 10%)

### Metrics for Model Success


```{r}

survey %>% 
  dplyr::select(contains("not your models were successful")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:5, key = "questions", value = "Metric") %>% 
  #tidyr::replace_na() %>% 
  group_by(Metric) %>% 
  count()  %>% 

  drop_na() %>% 

  hchart("column",hcaes("Metric","n")) %>% 
  hc_title(text = " Metrics used in organizations to determine whether or not your models were successful") %>% 
    hc_add_theme(hc_theme_darkunica())
```

* While it's obvious still a fact that Most people would say that usual `Model Accuracy` metrics are considered to determine whether a model is successful or not
* `Revenue / Business Goals` has almost 2X more votes than `Metri s that consider unfair Bias`


### Difficulty in implementing a Fair / Bias-free ML Model 


```{r}

survey %>% 
  dplyr::select(contains("difficult about ensuring that your algorithms are fair and unbiased")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:6, key = "questions", value = "Difficulty") %>% 
  #tidyr::replace_na() %>% 
  group_by(Difficulty) %>% 
  count()  %>% 

  drop_na() %>% 

  hchart("bar",hcaes("Difficulty","n")) %>% 
  hc_title(text = "Most difficult about ensuring that your algorithms are fair and unbiased?") %>% 
    hc_add_theme(hc_theme_darkunica())
```

* Collecting relevant **Data without Bias** is one of the most prominent difficulty in building a Bias free and Fair Model
* That is followed by, Picking the right **Evaluation Metric for the model's ML Bias** and **Identifying the group that's been unfairly targetted** in the data so that can be used to unbias / nullify the effect


# Model Interpretability / Explaining ML Models


### Who are they?

*They* refer to those beings who think *Being able to explain ML model outputs and/or predictions* are **Very Important** in Machine Learning and *Not They* are those who think otherwise including - Somewhat Important, Not at all important and similar.


```{r}

they <- survey %>% filter(`How do you perceive the importance of the following topics? - Being able to explain ML model outputs and/or predictions` == "Very important")


not_they <- survey %>% filter(`How do you perceive the importance of the following topics? - Being able to explain ML model outputs and/or predictions` != "Very important")

```


### Gender


```{r}
they %>% filter(!`What is your gender? - Selected Choice` %in% c("Other")) %>% 
  group_by(`What is your gender? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Degree" = `What is your gender? - Selected Choice`,
         "They" = n) %>%  
  bind_cols(
not_they %>% filter(!`What is your gender? - Selected Choice` %in% c("Other")) %>% 
  group_by(`What is your gender? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Degree1" = `What is your gender? - Selected Choice`,
         "Not They" = n) %>% 
  select(-Degree1)) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("column",hcaes("Degree","T_NT_Ratio")) %>% 
  hc_title(text = "Gender-wise preference - Interpretable Machine Learning  ") %>% 
  hc_add_theme(hc_theme_538()) 
  
```


### Age


```{r}


they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`What is your age (# years)?`) %>% count() %>% ungroup() %>% 
  rename("Age" = `What is your age (# years)?`,
         "They" = n) %>%  
  bind_cols(
not_they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`What is your age (# years)?`) %>% count() %>% ungroup() %>% 
  rename("Age1" = `What is your age (# years)?`,
         "Not They" = n) %>% 
  select(-Age1)) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("area",hcaes("Age","T_NT_Ratio")) %>% 
  hc_title(text = "Age Range - Interpretable Machine Learning  ") %>% 
  hc_add_theme(hc_theme_538()) 
  


```

* The cohort between Age group **18 - 29** are almost half of the index value of the cohort in **60-69** which shows how age and experience play a vital role in their perception about Interpretable Machine Learning 


### R vs Python vs more


```{r}

they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`What specific programming language do you use most often? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Lang" = `What specific programming language do you use most often? - Selected Choice`,
         "They" = n) %>%  
  bind_cols(
not_they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`What specific programming language do you use most often? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Lang1" = `What specific programming language do you use most often? - Selected Choice`,
         "Not They" = n) %>% 
  select(-Lang1)) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("bar",hcaes("Lang","T_NT_Ratio")) %>% 
  hc_title(text = "Language - Interpretable Machine Learning  ") %>% 
  hc_add_theme(hc_theme_538()) 
  

```

* Unexpectedly, **SAS/STATA** tops the index of They-Not_they ratio followed by **R** and **MATLAB** - making these three language users on Kaggle believe Model Intepretability is very important.

* **Python** users need to be made aware of IML as Python still catches up behind **SQL** and **Julia**.

### Degree - Education


```{r}
they %>% filter(!`Which best describes your undergraduate major? - Selected Choice` %in% c("Other")) %>% 
  group_by(`Which best describes your undergraduate major? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Degree" = `Which best describes your undergraduate major? - Selected Choice`,
         "They" = n) %>%  
  bind_cols(
not_they %>% filter(!`Which best describes your undergraduate major? - Selected Choice` %in% c("Other")) %>% 
  group_by(`Which best describes your undergraduate major? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Degree1" = `Which best describes your undergraduate major? - Selected Choice`,
         "Not They" = n) %>% 
  select(-Degree1)) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("column",hcaes("Degree","T_NT_Ratio")) %>% 
  hc_title(text = "Degree (Education) -  Interpretable Machine Learning ") %>% 
  hc_add_theme(hc_theme_538()) 
  
```

* Kagglers with **Humanities** background strongly believe in IML (Interpretable Machine Learning) with a TNT index of 2.28, followed by Engineerg (Non-CS) and **Mathematics/Statistitcs**
* Ironically, **Computer Science** Engineers are no where near the top with the index of 1.1764




### Work Industry

```{r warning=FALSE}

they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Industry" = `In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`,
         "They" = n) %>%  
  bind_cols(
not_they %>% #filter(`In which country do you currently reside?` %in% c("India","United States of America")) %>% 
  group_by(`In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`) %>% count() %>% ungroup() %>% 
  rename("Industry1" = `In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice`,
         "Not They" = n) %>% 
  select(-Industry1)) %>% 
  #filter((They + `Not They`) > 100) %>% 
  mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  filter(!Industry %in% c("3","Other")) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("line",hcaes("Industry","T_NT_Ratio")) %>% 
  hc_title(text = "IML - They vs Not They Ratio - Industry-wise") %>% 
    hc_add_theme(hc_theme_538())


```

* Kagglers from **Insurance** and **Military/Defense** industry background lead the table 
* As we've seen before, **Students** are no where close to perceive **Interpretable Machine Learning** is Very important
* Industries like **Marketing**, **Online Services** are worse than Students in their perception


### MOOC - Online course Platform


```{r}


they %>% 
  dplyr::select(contains("online platforms have you begun or completed data science courses")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:13, key = "questions", value = "DataType") %>% 
 
  #tidyr::replace_na() %>% 
  group_by(DataType) %>% 
  count() %>% 
  rename("They" = n) %>% 
bind_cols(
 

not_they %>% 
  dplyr::select(contains("online platforms have you begun or completed data science courses")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:13, key = "questions", value = "DataType") %>% 
 
  group_by(DataType) %>% 
  count() %>% 
  rename("Not They" = n)
) %>% 
  dplyr::select(-DataType1) %>% 
mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("area",hcaes("DataType","T_NT_Ratio")) %>% 
  hc_title(text = " IML - MOOC - Online course platform - They vs Not They Ratio") %>% 
    hc_add_theme(hc_theme_538())
```

* **Kaggle Learn** that has an exclusive course on **Interpretable Machine Learning** hasn't managed to be on the top, only with the 5 position
* **Datacamp** Kagglers have a strong feeling of **Very important** about **IML**
* Unlike, **Model Bias** perception where **Fast.ai** was on the top, here it's at the rock-bottom


### Media Sources


```{r}


they %>% 
  dplyr::select(contains("your favorite media sources that report on data science topics")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:22, key = "questions", value = "media") %>% 
 
  #tidyr::replace_na() %>% 
  group_by(media) %>% 
  count() %>% 
  rename("They" = n) %>% 
bind_cols(
 

not_they %>% 
  dplyr::select(contains("your favorite media sources that report on data science topics")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:22, key = "questions", value = "media") %>% 
 
  group_by(media) %>% 
  count() %>% 
  rename("Not They" = n)
) %>% 
  dplyr::select(-media1) %>% 
mutate("T_NT_Ratio" = round(They/`Not They`,3)) %>% 
  drop_na() %>% 
  arrange(desc(T_NT_Ratio)) %>% 
  hchart("line",hcaes("media","T_NT_Ratio")) %>% 
  hc_title(text = " IML - Media Sources - They vs Not They Ratio") %>% 
    hc_add_theme(hc_theme_538())
```

* **Kdnuggets**, **O'reily data newsletter** and **Kaggle Forum** top the list based on the index `T_NT_Ratio` making them one of the top media sources `They` get news from the most


### Exploring model insights? - Percentage of Data Projects


```{r}

survey %>% 
  select(`Approximately what percent of your data projects involve exploring model insights?`) %>% 
  drop_na() %>% 
  group_by(`Approximately what percent of your data projects involve exploring model insights?`) %>% 
  rename("Percentage" = `Approximately what percent of your data projects involve exploring model insights?`) %>% 
  count() %>% 
  hchart("area",hcaes("Percentage","n")) %>% 
  hc_title(text = "What percent of your data projects involve exploring model insights?") %>% 
  hc_add_theme(hc_theme_538()) 
  

```


### ML models to be black boxes - Difficult to explain


```{r}

survey %>% 
  select(`Do you consider ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?`) %>% 
  drop_na() %>% 
  group_by(`Do you consider ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?`) %>% 
  rename("Perspective" = `Do you consider ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?`) %>% 
  count() %>% 
  hchart("bar",hcaes("Perspective","n")) %>% 
  hc_title(text = 'ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?') %>% 
  hc_add_theme(hc_theme_538()) 
  

```

The above plot tells us that Most people are confident that they can understand and explain the outputs of Many ML models but not all ML models. In fact, those who feel - Most ML models are *Black Boxes* are more than those who don't have any opininon on this matter.




### Circumstances where exploring model insights and interpreting model happens


```{r}

survey %>% 
  dplyr::select(contains("circumstances would you explore model insights and interpret")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:6, key = "questions", value = "circumstances") %>% 
  #tidyr::replace_na() %>% 
  group_by(circumstances) %>% 
  count()  %>% 

  drop_na() %>% 

  hchart("bar",hcaes("circumstances","n")) %>% 
  hc_title(text = "Circumstances where exploring model insights and interpreting model happens") %>% 
    hc_add_theme(hc_theme_darkunica())
```


* The above plot indicates that this attempt to explore model insights and interpret model happens when **The model has been built specificially for that**

* That's followed by **When first exploring a new ML model or Dataset** which tells us an important point that **Model Intepretability** should be part of AMC or Model Refresh cycle rather than being the first-time duty. 



### Preferred Methods for Model Interpretability


```{r}

survey %>% 
  dplyr::select(contains("interpreting decisions that are made by ML models")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:15, key = "questions", value = "methods") %>% 
  #tidyr::replace_na() %>% 
  group_by(methods) %>% 
  count()  %>% 

  drop_na() %>% 

  hchart("bar",hcaes("methods","n")) %>% 
  hc_title(text = " Preferred explaining and/or interpreting decisions that are made by ML models") %>% 
    hc_add_theme(hc_theme_darkunica())
```

```{r warning=FALSE, message=FALSE}

survey %>% 
  select(`What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Other - Text`) %>% 
  drop_na() %>%
  rename("text" = `What methods do you prefer for explaining and/or interpreting decisions that are made by ML models? (Select all that apply) - Other - Text`) %>% 
  unnest_tokens(word, text)  %>% 
  anti_join(stop_words) %>%
  count(word) %>% 
  arrange(desc(n)) %>% 
  with(wordcloud(word, n, max.words = 100, colors = pal))
```



* Plotting **Predicted vs Actual Results** remain the standalone topper in the list of preferred methods in Interpretable Machine Learning which is kind of an irony because of the fact that it's an evaluation metric for Model succses rather than Interpreting Machine Learning Model
* IML technqiues like LIME, SHAP, ELI5 are way beyond in the list preferences, thus making a point of how much awareness the community has to be edicated to in terms of Interpretable Machine Learning Techniques



## Reproducibility 

### Tools and Methods for ML Reproduction

```{r}

survey %>% 
  dplyr::select(contains("tools and methods do you use to make your work easy to reproduce")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:11, key = "questions", value = "methods") %>% 
  #tidyr::replace_na() %>% 
  group_by(methods) %>% 
  count()  %>% 

  drop_na() %>% 

  hchart("bar",hcaes("methods","n")) %>% 
  hc_title(text = "Tools and methods do you use to make your work easy to reproduce") %>% 
    hc_add_theme(hc_theme_darkunica())
```


* **Documentation** and **Human-readable** codes lead the list of preferred tools and methods to make ML reproducible
* Sophisticated ways like **Docker** or **Virtual Box** are lot less preferred even ~ 50% lesser than those above easy options
* Sharing Code on **Github** and also code along with **Data** is another moderately preferred method, that's less expensive too
* Sharing code on Hosted Environment like **Kaggle Kernels** are in better position than Sophisticated techniques like **Docker** yet fall a lot behind than easy uploading on **Github**


### Barriers preventing from making easier to reuse and reproducible work

```{r}

survey %>% 
  dplyr::select(contains(" barriers prevent you from making your work even easier to reuse and reproduce")) %>% 
  dplyr::select(-contains("Text")) %>% 
  gather(1:8, key = "questions", value = "barriers") %>% 
  #tidyr::replace_na() %>% 
  group_by(barriers) %>% 
  count()  %>% 

  drop_na() %>% 

  hchart("bar",hcaes("barriers","n")) %>% 
  hc_title(text = "Barriers preventing from making easier to reuse and reproducible work") %>% 
    hc_add_theme(hc_theme_darkunica())
```


* Producing **Reproducible Machine Learning** works is considered to be **Too Time-consuming** which is the most prominent reason why Kagglers don't prefer doing so.
* Another important standing out point followed by the above one is, **Not having enough incentive of doing so** - reproducible Machine Learning.

# Recommendations from these Insights 

* Exposing **College Students** and Early in Career Data Scientsits to Model Fairness and Bias Machine Learning 
* Educating specially **Male** (and also female) Data scientsts the importance of unBiasing input training data / Model before moving it to production / presentation
* Suggesting places like **Kaggle and Bootcamps** to include Projects/Works about Model Fariness and Bias in their Data Science Curriculum
* Metrics to consider a model successful or not should include `ML unfair Bias evaluation metrics` too.
* Improving the avaialbilty of **Bias-free / Fair Dataset** for Machine Learning. Places like **Kaggle Dataset, UCI machine learning** should be advised to include **Fairness** as a label or option.
* While it's good that people who use Genetic Data are concerned about it, People who use **Image Data and Video data** could also be part of surveillance teams hence educating them about Model Bias could help a socially unbiased AI s  olution.
* **Kaggle Learn**, **Coursera** and **Udemy** need to improve their Course curriculum to include lessons about Model Bias and Fairness.
* Fast.AI, TheSchool.Ai need to have courses focused on Interpretable Machine Learning.
* To improve **Reprodcibility** in Machine Learning, it is essential to introduce tools that are as easy as uploading code on github but also that can have a completely reproducible project
* Coding Curriculum encouraging and training new comers using tools like **Rmarkdown** can help in better documentation
* Training on **Docker** and **Virtual Box** and encouraing to use **Kaggle Kernels/Collabs** should drive more reproducible work
* Ultimately, the **Incentive** of delivering a reproducible work should be applauded over **non-reproducible works**. In other words, **non-reproducilble work** must require extra scrutiny or lower-tier while **reproducible work** should have autoapproval work and verified tags as a sign of better incentive.

# Media Coverage about Bias / IML

* [Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)

# Undoing Bias / Improving IML - Papers and Attempts

* [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/abs/1607.06520)
* [Machine Learning and Human Bias - How Google does Machine Learning](https://www.coursera.org/lecture/google-machine-learning/machine-learning-and-human-bias-ZxQaP)
* [Google What If Tool - ML Probing Tool](https://pair-code.github.io/what-if-tool/)
* [Fairness in Machine Learning - NIPS 2017 Tutorial](https://mrtz.org/nips17/#/)
* [awesome-machine-learning-interpretability](https://github.com/jphall663/awesome-machine-learning-interpretability)
